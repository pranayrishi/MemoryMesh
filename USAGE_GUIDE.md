# MemoryMesh Usage Guide

## üöÄ Servers Running

‚úÖ **Backend**: http://localhost:5000  
‚úÖ **Frontend**: http://localhost:3000

## üìπ How the Video System Works

### Current Behavior (By Design)

1. **Initial State**: LiveMonitor shows **webcam feed** by default
2. **Demo Triggered**: When you click a scenario, it switches to **pre-generated AI video**
3. **Persona Detection**: System automatically detects grandma/grandpa from the video
4. **Intervention**: AI generates personalized response based on detected persona

### Why Webcam Shows First?

This is **intentional** - the system supports both modes:
- **Production mode**: Use webcam for real monitoring
- **Demo mode**: Use pre-generated videos when scenarios are triggered

The webcam will **automatically switch** to the demo video when you trigger a scenario.

## üé¨ How to Use Demo Scenarios

### Step 1: Open Dashboard
Navigate to http://localhost:3000 in your browser

### Step 2: Trigger a Scenario
1. Click the **‚ö° (lightning bolt)** button in the bottom-right corner
2. Select one of the 4 scenarios:
   - **Meal Confusion** - Patient checking fridge repeatedly
   - **Stove Safety** - Burner on with no pot (critical)
   - **Wandering** - Patient attempting to leave house
   - **Agitation** - Patient showing signs of distress

### Step 3: Watch the Demo
- Video feed switches from webcam to AI-generated video
- Persona detection overlay appears (grandma/grandpa with confidence %)
- AI intervention executes automatically
- Voice message plays through speakers
- Actions are displayed in the intervention card

## ‚ö†Ô∏è Current Status: Videos Not Generated Yet

The system is currently showing:
```
‚ö†Ô∏è  No pre-generated videos found
   Run: python video/generate_persona_videos.py
```

### What This Means:
- Demo scenarios will still work (using existing demo responses)
- BUT videos won't play (webcam will stay active)
- Persona detection won't work without videos

### To Enable Full Video Features:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY='your-key-here'

# Generate videos (one-time, ~30-60 minutes)
python video/generate_persona_videos.py
```

This will create 8 videos that the system will automatically use.

## üîç Understanding Interventions

### Why "No Action Taken" Might Show

The intervention card shows actions based on what the AI decides. If you see "no action taken", it could mean:

1. **AI determined no intervention needed** (patient is calm)
2. **Actions are in the decision but not displayed** (frontend issue)
3. **Demo response doesn't include explicit actions** (by design for some scenarios)

### Expected Actions for Each Scenario:

#### Meal Confusion (AI_ONLY)
- ‚úÖ Voice response to patient
- ‚úÖ Show timestamped meal photo
- ‚úÖ Show family photos
- ‚úÖ Play calming music
- ‚úÖ Redirect to bird watching

#### Stove Safety (EMERGENCY)
- üö® Voice response (calm, non-alarming)
- üö® Turn off stove via smart home
- üö® Redirect to bird watching
- üö® Play calming music
- üö® Notify caregiver (CRITICAL)

#### Wandering (NOTIFY)
- ‚ö†Ô∏è Voice response (validation)
- ‚ö†Ô∏è Show Hawaii trip photos
- ‚ö†Ô∏è Use reminiscence therapy
- ‚ö†Ô∏è Redirect to indoor seating
- ‚ö†Ô∏è Play favorite music
- ‚ö†Ô∏è Notify caregiver (MEDIUM)

#### Agitation (NOTIFY)
- ‚ö†Ô∏è Voice response (soothing)
- ‚ö†Ô∏è Play calming music (Frank Sinatra)
- ‚ö†Ô∏è Show grandchildren photos
- ‚ö†Ô∏è Create comfortable environment
- ‚ö†Ô∏è Dim lights
- ‚ö†Ô∏è Notify caregiver (MEDIUM)

## üêõ Troubleshooting

### WebSocket Connection Errors

If you see:
```
WebSocket connection to 'ws://localhost:3000/ws' failed
```

**This is normal** - it's from React's hot reload system, not your app. Your app uses `ws://localhost:5000` which should work fine.

### Camera Permission Denied

If the browser asks for camera permission:
- **Allow it** if you want to see the webcam feed initially
- **Deny it** if you only want to use demo videos (scenarios will still work)

### Actions Not Showing

Check the browser console (F12) for errors. The intervention should include:
- `intervention.decision.voice_message` - what AI says
- `intervention.actions[]` - array of action objects
- `intervention.notifications[]` - caregiver alerts

### Videos Not Playing

1. **Check if videos exist**:
   ```bash
   ls -la assets/videos/
   ```

2. **Generate videos if missing**:
   ```bash
   export OPENAI_API_KEY='your-key'
   python video/generate_persona_videos.py
   ```

3. **Verify video service**:
   ```bash
   curl http://localhost:5000/api/videos/status
   ```

## üéØ Testing the Full System

### Test 1: Basic Demo (Without Videos)
1. Open http://localhost:3000
2. Click ‚ö° button
3. Select "Meal Confusion"
4. **Expected**: 
   - Intervention card updates
   - Voice message displays
   - Actions show (if implemented correctly)
   - Statistics update

### Test 2: With Videos (After Generation)
1. Generate videos first
2. Open http://localhost:3000
3. Click ‚ö° button
4. Select any scenario
5. **Expected**:
   - Video feed switches to AI video
   - "DEMO" indicator appears
   - Persona overlay shows (grandma/grandpa)
   - Video loops continuously
   - Intervention executes
   - Actions display

### Test 3: Persona Detection
```bash
# After generating videos
python cv/persona_detector.py
```

Should show detection results for all 8 videos.

## üìä API Endpoints for Testing

### Check Backend Health
```bash
curl http://localhost:5000/api/health
```

### Check Video Status
```bash
curl http://localhost:5000/api/videos/status
```

### Check Available Videos
```bash
curl http://localhost:5000/api/videos/available
```

### Check Persona Detection Status
```bash
curl http://localhost:5000/api/persona/status
```

### Trigger Scenario Manually
```bash
curl -X POST http://localhost:5000/api/demo/scenario/meal_confusion
```

## üîÑ Current Workflow

```
1. User opens dashboard ‚Üí Webcam feed shows
2. User clicks ‚ö° button ‚Üí Demo panel opens
3. User selects scenario ‚Üí Backend processes request
4. Backend checks for video ‚Üí Selects appropriate video (if available)
5. Backend detects persona ‚Üí Identifies grandma/grandpa
6. Backend generates intervention ‚Üí Uses DemoResponseService
7. Backend broadcasts to frontend ‚Üí WebSocket message sent
8. Frontend updates:
   - Video feed switches to demo video
   - Persona overlay appears
   - Intervention card updates
   - Actions display
   - Statistics refresh
9. Voice plays through speakers ‚Üí ElevenLabs or Google Home
10. Timeline updates ‚Üí New event added
```

## üí° Key Points

### ‚úÖ What's Working Now (Without Videos)
- Backend server running
- Frontend dashboard loading
- Demo scenarios triggering
- AI intervention generation
- Voice synthesis (ElevenLabs/Google Home)
- WebSocket communication
- Statistics tracking
- Timeline updates

### ‚è≥ What Needs Videos
- Video playback in LiveMonitor
- Persona detection overlay
- Switching from webcam to demo video
- Visual demonstration of scenarios

### üé¨ After Generating Videos
Everything above PLUS:
- Pre-generated videos play automatically
- Persona detection works
- Grandma/grandpa identified correctly
- Full visual demo experience

## üöÄ Next Steps

1. **To use without videos**: Just use the dashboard as-is. Demos work, just no video playback.

2. **To enable full features**: Generate videos once:
   ```bash
   export OPENAI_API_KEY='your-key'
   python video/generate_persona_videos.py
   ```

3. **For production**: Upload videos to CDN, update VideoPlaybackService URLs.

## üìû Support

- **Backend logs**: Check terminal running `npm run server`
- **Frontend logs**: Open browser console (F12)
- **Video generation**: See `VIDEO_GENERATION_GUIDE.md`
- **Full implementation**: See `IMPLEMENTATION_SUMMARY.md`

---

**The system is fully functional right now. Video generation is optional but recommended for the enhanced visual demo experience.**
